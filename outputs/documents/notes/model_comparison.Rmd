---
title: "Model comparison"
author: "Keli Chiu"
date: "05/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## BERT

### Pros

* The Masked Language Model of BERT does text-infilling by its own mechanism, no fine-tuning is needed. 

* In contrast to autoregressive models such as GPT-2, BERT uses bidirectional contexts when predicting tokens, leading to improved performance.

### Cons

* The use of bidirectional attention limits its infilling capability to fixed-length spans. (Chris Donahue)

* The [MASK] symbol used by BERT during pre-training are not present in real data at fine-tuning time, resulting in a pretrain-finetune discrepancy.

* BERT is not able to model the joint probability and assumes the predicted tokens are independent of each other.


## GPT-2

### Pros

* GPT-2 does not rely on data corruption and therefore does not suffer from the pretrain-finetune discrepancy like BERT does

* Can be fine-tuned to do unstricted length of infilling

### Cons

* The inference time is relatively long.

* GPT-2 predicts the tokens based on previous words and does not take contexts on both side into account.

* According to Caleb Kaiser on Too big to deploy: How GPT-2 is breaking servers, GPT-2 is both compute hungry and memory hungry: "GPT-2 is compute hungry. In order to serve a single prediction, GPT-2 can occupy a CPU at 100% utilization for several minutes. Even with a GPU, a single prediction can still take seconds. ... GPT-2 is memory hungry. Beyond its considerable disk space and compute requirements, GPT-2 also needs large amounts of memory to run without crashing."

* Needs to be fine-tuned for text-infilling task. Fine-tuning would typically take 1 GPU a day.

## XLNet

### Pros

* Bidirectional contexts are taken into account when predicting tokens by calculating all possible permutations of the factorization order.

* XLNet does not rely on data corruption and therefore does not suffer from the pretrain-finetune discrepancy like BERT does.

* Can be fine-tuned to do unstricted length of infilling

### Cons

* According to Ajit Rajasekharan's post on Quora dated July 6, 2019, XLNet requires more compute power and memory (GPU/TPU memory) in comparison to BERT.

* Needs to be fine-tuned for text-infilling task. Fine-tuning would typically take 1 GPU a day.
