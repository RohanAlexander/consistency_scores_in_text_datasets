---
title: "Pytorch-Bert-Text-Infilling"
author: "Keli Chiu"
date: "17/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Calling Python in R by `reticulate`

The pretrained BERT model is from Pytorch, we are loading the necessary modules through `reticulate` library which permites operation of Python modules in R, and `rTorch`, an R interface to Pytorch. It's recommended to create a conda python environment for rTorch. This part is done in command line. `my-torch` is the name of the environment that we are going to tell R later, you can change it to something else.

``` {sh, eval = FALSE}
### Create a conda environment
conda create -n my-torch python=3.7 -y

### Activate the new environment with 
conda activate my-torch

### Inside the new environment, install PyTorch and related packages with:

conda install python=3.6 pytorch torchvision matplotlib pandas -c pytorch
```

Back to R, we load the `reticulate` library and tell it to use `my-torch` as the python environment. After that, we will import all the necessary modules and models.

``` {r}
library(reticulate)
use_condaenv("my-torch", required = TRUE) # Specify the python environment

pytorch_pretrained_bert <- import("pytorch_pretrained_bert")
BertModel <- pytorch_pretrained_bert$BertModel
BertForMaskedLM <- pytorch_pretrained_bert$BertForMaskedLM
BertTokenizer <- pytorch_pretrained_bert$BertTokenizer
tokenizer <- BertTokenizer$from_pretrained('bert-base-uncased')
model <- BertForMaskedLM$from_pretrained('bert-base-uncased')
```


## Load and clean text (to be included in the function)

The ground truth text is:

> "His Majesty Government wish to add that they have no intention of requesting the establishment of military bases in peace time within the area of Palestine now united to the Kingdom of Jordan."

The words "intention" and "Kingdom" are turned into errors:

> "His Majesty Government wish to add that they have no im@@@l@fon of requesting the establishment of military bases in peace time within the area of Palestine now united to the Kis@@®m of Jordan."

We load the text with errors and clean it by removing digits and special characters. "?", ".", "!" are kept as they help BERT to do the segmentation of sentences.

``` {r}
### Get text
text_original <- c("His Majesty Government wish to add that they have no im@@@l@fon of 
                   requesting the establishment of military bases in peace time within 
                   the area of Palestine now united to the Kis@@®m of Jordan.")

### Clean text
library(textclean)
text_stripped <- strip(text_original, char.keep = c("?", ".", "!"), digit.remove = TRUE, 
                       apostrophe.remove = FALSE, lower.case = FALSE)

text_stripped
```

## Identify errors and encode them (to be included in the function)

Right now, the error detection is done through a spellchecker as a start, real-word errors shall be tackled in the future. Once we have a list of errors, we replace them with a `[MASK]` token, which BERT recognizes as the token to be predicted.

``` {r}
### Collect text errors
library(hunspell)
errors <- hunspell(text_stripped)

### Replace errors to [MASK]
for(e in errors){
  pats <- paste(e, collapse = '|')
}
library(stringr)
text <- str_replace_all(text_stripped, pats, '[MASK]')

text
```

## Tokenize the text and encode them (to be included in the function)

We then tokenize the text and encode them. There are two layers of encoding:

* `segments_ids` is the encoding of the sentences

* `indexed_tokens` is the encoding of each word in the sentences

We then turn the above two into tensors as BERT's input by the function `torch$tensor`, namely `segments_tensors` and `tokens_tensor`.

``` {r}
### Tokenize the text
tokenized_text <- tokenizer$tokenize(text)

### Encode the text and create segments tensors
indexed_tokens <- tokenizer$convert_tokens_to_ids(tokenized_text)
SEGS <- which(tokenized_text == ".")
segments_ids <- rep(as.integer(0), times = SEGS)

### Get the position of the [MASK] tokens
MASKIDS <- which(tokenized_text == "[MASK]")

### Make the encoding into tensors as BERT inputs
library(rTorch)
segments_tensors <- torch$tensor(list(segments_ids))
tokens_tensor <- torch$tensor(list(indexed_tokens))

segments_tensors
tokens_tensor
```

## Model preduciton (to be included in the function)

We feed the `tokens_tensor` and `segments_tensors` in the model to produce prediction tensor.

``` {r}
### Generate prediction
py <- torch$no_grad()
with(py, {
  predictions <- model(tokens_tensor, segments_tensors)
})

predictions
```

## Prediction function

Here is a half-ly constructed function to produce the text prediction, the above scripts are to included to make the function complete. Here, we replace `[MASK]` token in the text to the predicted words, which are "intention" and "area".

``` {r, warning = FALSE}
### Prediction function
predict_tokens <- function(text, predictions, MASKIDS) {
  predicted_token <- list()
  for(i in 1:length(MASKIDS)){
    preds <- torch$topk(predictions[[0]][MASKIDS[i]], k=as.integer(50))
    indices <- preds$indices$tolist()
    list <- tokenizer$convert_ids_to_tokens(indices)
    predicted_token <- append(predicted_token, list(list[1]))
  }
  predicted_token.i <- 1

  for (i in 1:length(text)) {
    while (grepl(pattern = "\\Q[MASK]\\E", text[i])) {
      text[i] <- sub(pattern = "\\Q[MASK]\\E", replacement = predicted_token[predicted_token.i], x = text[i])
      predicted_token.i <- predicted_token.i + 1
    }
  }

  return(text)
}

predict_tokens(text, predictions, MASKIDS)
```
