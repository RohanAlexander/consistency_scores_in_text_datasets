,id,abstract
0,0,"Turing machines and G\""odel numbers are important pillars of the theory of computation. Thus, any computational architecture needs to show how it could relate to Turing machines and how stable implementations of Turing computation are possible. In this chapter, we implement universal Turing computation in a neural field environment. To this end, we employ the canonical symbologram representation of a Turing machine obtained from a G\""odel encoding of its symbolic repertoire and generalized shifts. The resulting nonlinear dynamical automaton (NDA) is a piecewise affine-linear map acting on the unit square that is partitioned into rectangular domains. Instead of looking at point dynamics in phase space, we then consider functional dynamics of probability distributions functions (p.d.f.s) over phase space. This is generally described by a Frobenius-Perron integral transformation that can be regarded as a neural field equation over the unit square as feature space of a dynamic field theory (DFT). Solving the Frobenius-Perron equation yields that uniform p.d.f.s with rectangular support are mapped onto uniform p.d.f.s with rectangular support, again. We call the resulting representation \emph{dynamic field automaton}."
1,1,"RNA-sequencing has revolutionized biomedical research and, in particular, our ability to study gene alternative splicing. The problem has important implications for human health, as alternative splicing may be involved in malfunctions at the cellular level and multiple diseases. However, the high-dimensional nature of the data and the existence of experimental biases pose serious data analysis challenges. We find that the standard data summaries used to study alternative splicing are severely limited, as they ignore a substantial amount of valuable information. Current data analysis methods are based on such summaries and are hence suboptimal. Further, they have limited flexibility in accounting for technical biases. We propose novel data summaries and a Bayesian modeling framework that overcome these limitations and determine biases in a nonparametric, highly flexible manner. These summaries adapt naturally to the rapid improvements in sequencing technology. We provide efficient point estimates and uncertainty assessments. The approach allows to study alternative splicing patterns for individual samples and can also be the basis for downstream analyses. We found a severalfold improvement in estimation mean square error compared popular approaches in simulations, and substantially higher consistency between replicates in experimental data. Our findings indicate the need for adjusting the routine summarization and analysis of alternative splicing RNA-seq studies. We provide a software implementation in the R package casper."
2,2,"Queuing models provide insight into the temporal inhomogeneity of human dynamics, characterized by the broad distribution of waiting times of individuals performing tasks. We study the queuing model of an agent trying to execute a task of interest, the priority of which may vary with time due to the agent's ""state of mind."" However, its execution is disrupted by other tasks of random priorities. By considering the priority of the task of interest either decreasing or increasing algebraically in time, we analytically obtain and numerically confirm the bimodal and unimodal waiting time distributions with power-law decaying tails, respectively. These results are also compared to the updating time distribution of papers in the arXiv.org and the processing time distribution of papers in Physical Review journals. Our analysis helps to understand human task execution in a more realistic scenario."
3,3,"In a multiple-object auction, every bidder tries to win as many objects as possible with a bidding algorithm. This paper studies position-randomized auctions, which form a special class of multiple-object auctions where a bidding algorithm consists of an initial bid sequence and an algorithm for randomly permuting the sequence. We are especially concerned with situations where some bidders know the bidding algorithms of others. For the case of only two bidders, we give an optimal bidding algorithm for the disadvantaged bidder. Our result generalizes previous work by allowing the bidders to have unequal budgets. One might naturally anticipate that the optimal expected numbers of objects won by the bidders would be proportional to their budgets. Surprisingly, this is not true. Our new algorithm runs in optimal O(n) time in a straightforward manner. The case with more than two bidders is open."
4,4,"In arXiv:1109.6438v1 [math.AG] we introduced and studied a notion of algebraic entropy. In this paper we will give an application of algebraic entropy in proving Kunz Regularity Criterion for all contracting self-maps of finite length of Noetherian local rings in arbitrary characteristic. Some conditions of Kunz Criterion have already been extended to the general case by Avramov, Iyengar and Miller in arXiv:math/0312412v2 [math.AC], using different methods."
5,5,"We consider a set of k autonomous robots that are endowed with visibility sensors (but that are otherwise unable to communicate) and motion actuators. Those robots must collaborate to reach a sin- gle vertex that is unknown beforehand, and to remain there hereafter. Previous works on gathering in ring-shaped networks suggest that there exists a tradeoff between the size of the set of potential initial configurations, and the power of the sensing capabilities of the robots (i.e. the larger the initial configuration set, the most powerful the sensor needs to be). We prove that there is no such trade off. We propose a gathering protocol for an odd number of robots in a ring-shaped network that allows symmetric but not periodic configurations as initial configurations, yet uses only local weak multiplicity detection. Robots are assumed to be anonymous and oblivious, and the execution model is the non-atomic CORDA model with asynchronous fair scheduling. Our protocol allows the largest set of initial configurations (with respect to impossibility results) yet uses the weakest multiplicity detector to date. The time complexity of our protocol is O(n2), where n denotes the size of the ring. Compared to previous work that also uses local weak multiplicity detection, we do not have the constraint that k < n/2 (here, we simply have 2 < k < n - 3)."
6,6,"The Fermilab Linac delivers a variable intensity, 400-MeV beam to the The MuCool Test Area experimental hall via a beam line specifically designed to facilitate measurements of the Linac beam emittance and properties. A 10 m, dispersion-free and magnet-free straight utilizes an upstream quadrupole focusing triplet in combination with the necessary in-straight beam diagnostics to fully characterize the transverse beam properties. Since the Linac does not produce a strictly elliptical phase space, tomography must be performed on the profile data to retrieve the actual particle distribution in phase space. This is achieved by rotating the phase space distribution using different waist focusing conditions of the upstream triplet and performing a de-convolution of the profile data. Preliminary measurements using this diagnostic section are reported here."
7,7,"Let R be the quotient of a polynomial ring over a field k by an ideal generated by monomials. We derive a formula for the multigraded Poincare' series of R, i.e., the generating function for the ranks of the modules in a minimal multigraded free resolution of k over R. The formula can be expressed in terms of the homology of lower intervals in a certain lattice associated to the minimal set of generators for the ideal."
8,8,"Discontinuity preserving smoothing is a fundamentally important procedure that is useful in a wide variety of image processing contexts. It is directly useful for noise reduction, and frequently used as an intermediate step in higher level algorithms. For example, it can be particularly useful in edge detection and segmentation. Three well known algorithms for discontinuity preserving smoothing are nonlinear anisotropic diffusion, bilateral filtering, and mean shift filtering. Although slight differences make them each better suited to different tasks, all are designed to preserve discontinuities while smoothing. However, none of them satisfy this goal perfectly: they each have exception cases in which smoothing may occur across hard edges. The principal contribution of this paper is the identification of a property we call edge awareness that should be satisfied by any discontinuity preserving smoothing algorithm. This constraint can be incorporated into existing algorithms to improve quality, and usually has negligible changes in runtime performance and/or complexity. We present modifications necessary to augment diffusion and mean shift, as well as a new formulation of the bilateral filter that unifies the spatial and range spaces to achieve edge awareness."
9,9,"We investigate the properties of the Hybrid Monte-Carlo algorithm (HMC) in high dimensions. HMC develops a Markov chain reversible w.r.t. a given target distribution $\Pi$ by using separable Hamiltonian dynamics with potential $-\log\Pi$. The additional momentum variables are chosen at random from the Boltzmann distribution and the continuous-time Hamiltonian dynamics are then discretised using the leapfrog scheme. The induced bias is removed via a Metropolis-Hastings accept/reject rule. In the simplified scenario of independent, identically distributed components, we prove that, to obtain an $\mathcal{O}(1)$ acceptance probability as the dimension $d$ of the state space tends to $\infty$, the leapfrog step-size $h$ should be scaled as $h= l \times d^{-1/4}$. Therefore, in high dimensions, HMC requires $\mathcal{O}(d^{1/4})$ steps to traverse the state space. We also identify analytically the asymptotically optimal acceptance probability, which turns out to be 0.651 (to three decimal places). This is the choice which optimally balances the cost of generating a proposal, which {\em decreases} as $l$ increases, against the cost related to the average number of proposals required to obtain acceptance, which {\em increases} as $l$ increases."
10,10,"Consider an Erd\""os-Renyi random graph in which each edge is present independently with probability 1/2, except for a subset $\sC_N$ of the vertices that form a clique (a completely connected subgraph). We consider the problem of identifying the clique, given a realization of such a random graph.   The best known algorithm provably finds the clique in linear time with high probability, provided $|\sC_N|\ge 1.261\sqrt{N}$ \cite{dekel2011finding}. Spectral methods can be shown to fail on cliques smaller than $\sqrt{N}$. In this paper we describe a nearly linear time algorithm that succeeds with high probability for $|\sC_N|\ge (1+\eps)\sqrt{N/e}$ for any $\eps>0$. This is the first algorithm that provably improves over spectral methods.   We further generalize the hidden clique problem to other background graphs (the standard case corresponding to the complete graph on $N$ vertices). For large girth regular graphs of degree $(\Delta+1)$ we prove that `local' algorithms succeed if $|\sC_N|\ge (1+\eps)N/\sqrt{e\Delta}$ and fail if $|\sC_N|\le(1-\eps)N/\sqrt{e\Delta}$."
11,11,"We show how to exploit symmetries of a graph to efficiently compute the fastest mixing Markov chain on the graph (i.e., find the transition probabilities on the edges to minimize the second-largest eigenvalue modulus of the transition probability matrix). Exploiting symmetry can lead to significant reduction in both the number of variables and the size of matrices in the corresponding semidefinite program, thus enable numerical solution of large-scale instances that are otherwise computationally infeasible. We obtain analytic or semi-analytic results for particular classes of graphs, such as edge-transitive and distance-transitive graphs. We describe two general approaches for symmetry exploitation, based on orbit theory and block-diagonalization, respectively. We also establish the connection between these two approaches."
12,12,We present a discussion on some physical aspects of gravitational collapse which is based on a list of questions related to relevant issues in the study of that phenomenon. Providing answers to those questions we bring out the role played by different physical processes in the dynamics of spherical collapse.
13,13,"SOM is a type of unsupervised learning where the goal is to discover some underlying structure of the data. In this paper, a new extraction method based on the main idea of Concurrent Self-Organizing Maps (CSOM), representing a winner-takes-all collection of small SOM networks is proposed. Each SOM of the system is trained individually to provide best results for one class only. The experiments confirm that the proposed features based CSOM is capable to represent image content better than extracted features based on a single big SOM and these proposed features improve the final decision of the CAD. Experiments held on Mammographic Image Analysis Society (MIAS) dataset."
14,14,"After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error."
15,15,"The ever-increasing quantity and complexity of scientific production have made it difficult for researchers to keep track of advances in their own fields. This, together with growing popularity of online scientific communities, calls for the development of effective information filtering tools. We propose here a method to simultaneously compute reputation of users and quality of scientific artifacts in an online scientific community. Evaluation on artificially-generated data and real data from the Econophysics Forum is used to determine the method's best-performing variants. We show that when the method is extended by considering author credit, its performance improves on multiple levels. In particular, top papers have higher citation count and top authors have higher $h$-index than top papers and top authors chosen by other algorithms."
16,16,"This paper reviews recent developments of robust estimation in linear time series models, with short and long memory correlation structures, in the presence of additive outliers. Based on the manuscripts Fajardo et al. (2009) and L\'evy-Leduc et al. (2011a), the emphasis in this paper is given in the following directions; the influence of additive outliers in the estimation of a time series, the asymptotic properties of a robust autocovariance function and a robust semiparametric estimation method of the fractional parameter d in ARFIMA(p, d, q) models. Some simulations are used to support the use of the robust method when a time series has additive outliers. The invariance property of the estimators for the first difference in ARFIMA model with outliers is also discussed. In general, the robust long-memory estimator leads to be outlier resistant and is invariant to first differencing."
17,17,"The principal time properties - the one-dimensionality and the irreversibility -, the space metric properties and the spatial-temporal principles of the theory of the relativity are deduced from three natural logic properties of the information, obtained by a physics device. Hence, the transformations of the complete Poincare group are deduced from that."
18,18,"Main characteristics of Google Scholar Metrics new version (july 2013) are presented. We outline the novelties and the weaknesses detected after a first analysis. As main conclusion, we remark the lack of new functionalities with respect to last editions, as the only modification is the update of the timeframe (2008-2012). Hence, problems pointed out in our last reviews still remain active. Finally, it seems Google Scholar Metrics will be updated in a yearly basis"
19,19,"Gaussian graphical models represent the backbone of the statistical toolbox for analyzing continuous multivariate systems. However, due to the intrinsic properties of the multivariate normal distribution, use of this model family may hide certain forms of context-specific independence that are natural to consider from an applied perspective. Such independencies have been earlier introduced to generalize discrete graphical models and Bayesian networks into more flexible model families. Here we adapt the idea of context-specific independence to Gaussian graphical models by introducing a stratification of the Euclidean space such that a conditional independence may hold in certain segments but be absent elsewhere. It is shown that the stratified models define a curved exponential family, which retains considerable tractability for parameter estimation and model selection."
20,20,"Recently, connections have been explored between the complexity of finite problems in graph theory and the complexity of their infinite counterparts. As is shown in our paper (and in independent work of Tirza Hirst and D. Harel from a different angle) there is no firm connection between these complexities, namely finite problems of equal complexity can have radically different complexity for the infinite versions and vice versa. Furthermore, the complexity of an infinite counterpart can depend heavily on precisely how the finite problem is rephrased in the infinite case.   The finite problems we address include colorability of graphs and existence of subgraph isomorphisms. In particular, we give three infinite versions of the 3-colorability problem that vary considerably in their recursion theoretic and proof theoretic complexity. Additionally, we show that three subgraph isomorphism problems of varying finite complexity have infinite versions with identical recursion theoretic and proof theoretic content."
21,21,"We present a generic approach for treating the effect of nuclear motion in the high-order harmonic generation from polyatomic molecules. Our procedure relies on a separation of nuclear and electron dynamics where we account for the electronic part using the Lewenstein model and nuclear motion enters as a nuclear correlation function. We express the nuclear correlation function in terms of Franck-Condon factors which allows us to decompose nuclear motion into modes and identify the modes that are dominant in the high-order harmonic generation process. We show results for the isotopes CH$_4$ and CD$_4$ and thereby provide direct theoretical support for a recent experiment [Baker {\it et al.}, Science {\bf 312}, 424 (2006)] that uses high-order harmonic generation to probe the ultra-fast structural nuclear rearrangement of ionized methane."
22,22,"The problem of distributed learning and channel access is considered in a cognitive network with multiple secondary users. The availability statistics of the channels are initially unknown to the secondary users and are estimated using sensing decisions. There is no explicit information exchange or prior agreement among the secondary users. We propose policies for distributed learning and access which achieve order-optimal cognitive system throughput (number of successful secondary transmissions) under self play, i.e., when implemented at all the secondary users. Equivalently, our policies minimize the regret in distributed learning and access. We first consider the scenario when the number of secondary users is known to the policy, and prove that the total regret is logarithmic in the number of transmission slots. Our distributed learning and access policy achieves order-optimal regret by comparing to an asymptotic lower bound for regret under any uniformly-good learning and access policy. We then consider the case when the number of secondary users is fixed but unknown, and is estimated through feedback. We propose a policy in this scenario whose asymptotic sum regret which grows slightly faster than logarithmic in the number of transmission slots."
23,23,"Handling visual complexity is a challenging problem in visualization owing to the subjectiveness of its definition and the difficulty in devising generalizable quantitative metrics. In this paper we address this challenge by measuring the visual complexity of two common forms of cluster-based visualizations: scatter plots and parallel coordinatess. We conceptualize visual complexity as a form of visual uncertainty, which is a measure of the degree of difficulty for humans to interpret a visual representation correctly. We propose an algorithm for estimating visual complexity for the aforementioned visualizations using Allen's interval algebra. We first establish a set of primitive 2-cluster cases in scatter plots and another set for parallel coordinatess based on symmetric isomorphism. We confirm that both are the minimal sets and verify the correctness of their members computationally. We score the uncertainty of each primitive case based on its topological properties, including the existence of overlapping regions, splitting regions and meeting points or edges. We compare a few optional scoring schemes against a set of subjective scores by humans, and identify the one that is the most consistent with the subjective scores. Finally, we extend the 2-cluster measure to k-cluster measure as a general purpose estimator of visual complexity for these two forms of cluster-based visualization."
24,24,"Spiking Neural P systems, SNP systems for short, are biologically inspired computing devices based on how neurons perform computations. SNP systems use only one type of symbol, the spike, in the computations. Information is encoded in the time differences of spikes or the multiplicity of spikes produced at certain times. SNP systems with delays (associated with rules) and those without delays are two of several Turing complete SNP system variants in literature. In this work we investigate how restricted forms of SNP systems with delays can be simulated by SNP systems without delays. We show the simulations for the following spike routing constructs: sequential, iteration, join, and split."
25,25,"We propose maximum likelihood estimation for learning Gaussian graphical models with a Gaussian (ell_2^2) prior on the parameters. This is in contrast to the commonly used Laplace (ell_1) prior for encouraging sparseness. We show that our optimization problem leads to a Riccati matrix equation, which has a closed form solution. We propose an efficient algorithm that performs a singular value decomposition of the training data. Our algorithm is O(NT^2)-time and O(NT)-space for N variables and T samples. Our method is tailored to high-dimensional problems (N gg T), in which sparseness promoting methods become intractable. Furthermore, instead of obtaining a single solution for a specific regularization parameter, our algorithm finds the whole solution path. We show that the method has logarithmic sample complexity under the spiked covariance model. We also propose sparsification of the dense solution with provable performance guarantees. We provide techniques for using our learnt models, such as removing unimportant variables, computing likelihoods and conditional distributions. Finally, we show promising results in several gene expressions datasets."
26,26,We investigate the effect of shear in the flow of charged particle equilibria that are unstable to the Coherent Synchrotron Radiation (CSR) instability.   Shear may act to quench this instability because it acts to limit the size of the region with a fixed phase relation between emitters.   The results are important for the understanding of astrophysical sources of coherent radiation where shear in the flow is likely.
27,27,"In this paper, concept of possibility neutrosophic soft set and its operations are defined, and their properties are studied. An application of this theory in decision making is investigated. Also a similarity measure of two possibility neutrosophic soft sets is introduced and discussed. Finally an application of this similarity measure is given to select suitable person for position in a firm."
28,28,In this paper we establish a Hermite- Hadamard type inequality for operator preinvex functions and an estimate of the right hand side of a Hermite- Hadamard type inequality in which some operator preinvex functions of selfadjoint operators in Hilbert spaces are involved.
29,29,"After negative temperature is restated, we find that it will derive necessarily decrease of entropy. Negative temperature is based on the Kelvin scale and the condition dU>0 and dS<0. Conversely, there is also negative temperature for dU<0 and dS>0. But, negative temperature is contradiction with usual meaning of temperature and with some basic concepts of physics and mathematics. It is a question in nonequilibrium thermodynamics. We proposed a possibility of decrease of entropy due to fluctuation magnified and internal interactions in some isolated systems. From this we discuss some possible examples and theories."
30,30,"In many important applications -- such as search engines and relational database systems -- data is stored in the form of arrays of integers. Encoding and, most importantly, decoding of these arrays consumes considerable CPU time. Therefore, substantial effort has been made to reduce costs associated with compression and decompression. In particular, researchers have exploited the superscalar nature of modern processors and SIMD instructions. Nevertheless, we introduce a novel vectorized scheme called SIMD-BP128 that improves over previously proposed vectorized approaches. It is nearly twice as fast as the previously fastest schemes on desktop processors (varint-G8IU and PFOR). At the same time, SIMD-BP128 saves up to 2 bits per integer. For even better compression, we propose another new vectorized scheme (SIMD-FastPFOR) that has a compression ratio within 10% of a state-of-the-art scheme (Simple-8b) while being two times faster during decoding."
31,31,"We extend the Falceto-Zambon version of Marsden-Ratiu Poisson reduction to Poisson quasi-Nijenhuis structures with background on manifolds. We define gauge transformations of Poisson quasi-Nijenhuis structures with background, study some of their properties and show that they are compatible with reduction procedure. We use gauge transformations to construct Poisson quasi-Nijenhuis structures with background."
32,32,"This paper gives an account of our progress towards performing femtosecond time-resolved photoelectron diffraction on gas-phase molecules in a pump-probe setup combining optical lasers and an X-ray Free-Electron Laser. We present results of two experiments aimed at measuring photoelectron angular distributions of laser-aligned 1-ethynyl-4-fluorobenzene (C8H5F) and dissociating, laseraligned 1,4-dibromobenzene (C6H4Br2) molecules and discuss them in the larger context of photoelectron diffraction on gas-phase molecules. We also show how the strong nanosecond laser pulse used for adiabatically laser-aligning the molecules influences the measured electron and ion spectra and angular distributions, and discuss how this may affect the outcome of future time-resolved photoelectron diffraction experiments."
33,33,"Persistent homology is a widely used tool in Topological Data Analysis that encodes multiscale topological information as a multi-set of points in the plane called a persistence diagram. It is difficult to apply statistical theory directly to a random sample of diagrams. Instead, we can summarize the persistent homology with the persistence landscape, introduced by Bubenik, which converts a diagram into a well-behaved real-valued function. We investigate the statistical properties of landscapes, such as weak convergence of the average landscapes and convergence of the bootstrap. In addition, we introduce an alternate functional summary of persistent homology, which we call the silhouette, and derive an analogous statistical theory."
34,34,"Almost sure asymptotic stabilization of a discrete-time switched stochastic system is investigated. Information on the active operation mode of the switched system is assumed to be available for control purposes only at random time instants. We propose a stabilizing feedback control framework that utilizes the information obtained through mode observations. We first consider the case where stochastic properties of mode observation instants are fully known. We obtain sufficient asymptotic stabilization conditions for the closed-loop switched stochastic system under our proposed control law. We then explore the case where exact knowledge of the stochastic properties of mode observation instants is not available. We present a set of alternative stabilization conditions for this case. The results for both cases are predicated on the analysis of a sequence-valued process that encapsulates the stochastic nature of the evolution of active operation mode between mode observation instants. Finally, we demonstrate the efficacy of our results with numerical examples."
35,35,"Over the last few decades, climate scientists have devoted much effort to the development of large numerical models of the atmosphere and the ocean. While there is no question that such models provide important and useful information on complicated aspects of atmosphere and ocean dynamics, skillful prediction also requires a phenomenological approach, particularly for very slow processes, such as glacial-interglacial cycles. Phenomenological models are often represented as low-order dynamical systems. These are tractable, and a rich source of insights about climate dynamics, but they also ignore large bodies of information on the climate system, and their parameters are generally not operationally defined. Consequently, if they are to be used to predict actual climate system behaviour, then we must take very careful account of the uncertainty introduced by their limitations. In this paper we consider the problem of the timing of the next glacial inception, about which there is on-going debate. Our model is the three-dimensional stochastic system of Saltzman and Maasch (1991), and our inference takes place within a Bayesian framework that allows both for the limitations of the model as a description of the propagation of the climate state vector, and for parametric uncertainty. Our inference takes the form of a data assimilation with unknown static parameters, which we perform with a variant on a Sequential Monte Carlo technique (`particle filter'). Provisional results indicate peak glacial conditions in 60,000 years."
36,36,"We introduce the notion of graphic cocircuits and show that a large class of regular matroids with graphic cocircuits belongs to the class of signed-graphic matroids. Moreover, we provide an algorithm which determines whether a cographic matroid with graphic cocircuits is signed-graphic or not."
37,37,We have investigated the non-static Lorentzian Wormhole model in presence of anisotropic pressure. We have presented some exact solutions of Einstein equations for anisotropic pressure case. Introducing two EoS parameters we have shown that these solutions give very rich dynamics of the universe yielding to the different expansion history of it in the $r$ - direction and in the $T$ - direction. The corresponding explicit forms of the shape function $b(r)$ is presented.We have shown that the Einstein's field equations and unified first law are equivalent for the dynamical wormhole model. The first law of thermodynamics has been derived by using the Unified first law. The physical quantities including surface gravity and the temperature are derived for the wormhole. Here we have obtained all the results without any choice of the shape function. The validity of generalized second law (GSL) of thermodynamics has been examined at apparent and event horizons for the evolving Lorentzian wormhole.
38,38,"We study losses for binary classification and class probability estimation and extend the understanding of them from margin losses to general composite losses which are the composition of a proper loss with a link function. We characterise when margin losses can be proper composite losses, explicitly show how to determine a symmetric loss in full from half of one of its partial losses, introduce an intrinsic parametrisation of composite binary losses and give a complete characterisation of the relationship between proper losses and ``classification calibrated'' losses. We also consider the question of the ``best'' surrogate binary loss. We introduce a precise notion of ``best'' and show there exist situations where two convex surrogate losses are incommensurable. We provide a complete explicit characterisation of the convexity of composite binary losses in terms of the link function and the weight function associated with the proper loss which make up the composite loss. This characterisation suggests new ways of ``surrogate tuning''. Finally, in an appendix we present some new algorithm-independent results on the relationship between properness, convexity and robustness to misclassification noise for binary losses and show that all convex proper losses are non-robust to misclassification noise."
39,39,"In this paper we consider a semigroup of completely positive maps $\tau=(\tau_t,t \ge 0)$ with a faithful normal invariant state $\phi$ on a type-$II_1$ factor $\cla_0$ and propose an index theory. We :achieve this via a more general Kolmogorov's type of construction for stationary Markov processes which naturally associate a nested isomorphic von-Neumann algebras. In particular this construction generalizes well known Jones construction associated with a sub-factor of type-II$_1$ factor."
40,40,"Sparse systems are usually parameterized by a tuning parameter that determines the sparsity of the system. How to choose the right tuning parameter is a fundamental and difficult problem in learning the sparse system. In this paper, by treating the the tuning parameter as an additional dimension, persistent homological structures over the parameter space is introduced and explored. The structures are then further exploited in speeding up the computation using the proposed soft-thresholding technique. The topological structures are further used as multivariate features in the tensor-based morphometry (TBM) in characterizing white matter alterations in children who have experienced severe early life stress and maltreatment. These analyses reveal that stress-exposed children exhibit more diffuse anatomical organization across the whole white matter region."
41,41,"The existence of incentive-compatible computationally-efficient protocols for combinatorial auctions with decent approximation ratios is the paradigmatic problem in computational mechanism design. It is believed that in many cases good approximations for combinatorial auctions may be unattainable due to an inherent clash between truthfulness and computational efficiency. However, to date, researchers lack the machinery to prove such results. In this paper, we present a new approach that we believe holds great promise for making progress on this important problem. We take the first steps towards the development of new technologies for lower bounding the VC dimension of k-tuples of disjoint sets. We apply this machinery to prove the first computational-complexity inapproximability results for incentive-compatible mechanisms for combinatorial auctions. These results hold for the important class of VCG-based mechanisms, and are based on the complexity assumption that NP has no polynomial-size circuits."
42,42,"In this contribution we present a decentralized power allocation algorithm for the uplink interleave division multiple access (IDMA) channel. Within the proposed optimal strategy for power allocation, each user aims at selfishly maximizing its own utility function. An iterative chip by chip (CBC) decoder at the receiver and a rational selfish behavior of all the users according to a classical game-theoretical framework are the underlying assumptions of this work. This approach leads to a power allocation based on a channel inversion policy where the optimal power level is set locally at each terminal based on the knowledge of its own channel realization, the noise level at the receiver and the number of active users in the network."
43,43,"We establish a uniform comparison between the spectrum of the rough Laplacian (acting on sections of a vector bundle of complex rank one or of harmonic curvature) with the spectrum of a discrete operator (a generalization of a discrete magnetic Laplacian added with a potential) acting on a finite dimensional space coming from a discretization process. As an application, we obtain a comparison of the first positive eigenvalue of the rough Laplacian with the holonomy."
44,44,"We call a CNF formula linear if any two clauses have at most one variable in common. Let m(k) be the largest integer m such that any linear k-CNF formula with <= m clauses is satisfiable. We show that 4^k / (4e^2k^3) <= m(k) < ln(2) k^4 4^k. More generally, a (k,d)-CSP is a constraint satisfaction problem in conjunctive normal form where each variable can take on one of d values, and each constraint contains k variables and forbids exacty one of the d^k possible assignments to these variables. Call a (k,d)-CSP l-disjoint if no two distinct constraints have l or more variables in common. Let m_l(k,d) denote the largest integer m such that any l-disjoint (k,d)-CSP with at most m constraints is satisfiable. We show that 1/k (d^k/(ed^(l-1)k))^(1+1/(l-1))<= m_l(k,d) < c (k^2/l ln(d) d^k)^(1+1/(l-1)). for some constant c. This means for constant l, upper and lower bound differ only in a polynomial factor in d and k."
45,45,"We study the effects of higher order transversal modes in a model of a singly-resonant OPO, using both numerical solutions and mode expansions including up to two radial modes. The numerical and two-mode solutions predict lower threshold and higher conversion than the single-mode solution at negative dispersion. Relative power in the zero order radial mode ranges from about 88% at positive and small negative dispersion to 48% at larger negative dispersion, with most of the higher mode content in the first mode, and less than 2% in higher modes."
46,46,"A hypotheses of energy loss for polarization of e-e+ vacuum by a photon passing interstellar space is considered. An excitation and relaxation of vacuum can't run with speed of light due to very small but finite fraction of e-e+ pair mass that creates a retardment in recuperation of deposited energy back to photon. This ""forgotten"" by many photons energy is finally splashed out in real space as a Relic Radiation. An assumption that such energy loss is proportional to a photon energy conforms to Hubble low of Red Shift and experimental data treated as accelerated expansion of Universe. A possibility of an observation of this type energy loss is considered at high-energy accelerators where energy deposition may reach up hundreds MeV in second."
47,47,"We show for an arbitrary $\ell_p$ norm that the property that a random geometric graph $\mathcal G(n,r)$ contains a Hamiltonian cycle exhibits a sharp threshold at $r=r(n)=\sqrt{\frac{\log n}{\alpha_p n}}$, where $\alpha_p$ is the area of the unit disk in the $\ell_p$ norm. The proof is constructive and yields a linear time algorithm for finding a Hamiltonian cycle of $\RG$ a.a.s., provided $r=r(n)\ge\sqrt{\frac{\log n}{(\alpha_p -\epsilon)n}}$ for some fixed $\epsilon > 0$."
48,48,A theoretical thermodynamic cycle more efficient than an infinite set of Carnot engines is presented. This result is unexpected from the point of view of classical thermodynamics.
49,49,"Here we propose a new design paradigm for a superconducting nanowire single photon detector that uses a multi-layer architecture that places the electric leads beneath the nanowires. This allows for a very large number of detector elements, which we will call pixels in analogy to a conventional CCD camera, to be placed in close proximity. This leads to significantly better photon number resolution than current single and multi-nanowire meanders, while maintaining similar detection areas. We discuss the reset time of the pixels and how the design can be modified to avoid the latching failure seen in extremely short superconducting nanowires. These advantages give a multi-layer superconducting number-resolving photon detector significant advantages over the current design paradigm of long superconducting nanowire meanders. Such advantages are desirable in a wide array of photonics applications."
50,50,"We consider the counting rate estimation of an unknown radioactive source, which emits photons at times modeled by an homogeneous Poisson process. A spectrometer converts the energy of incoming photons into electrical pulses, whose number provides a rough estimate of the intensity of the Poisson process. When the activity of the source is high, a physical phenomenon known as pileup effect distorts direct measurements, resulting in a significant bias to the standard estimators of the source activities used so far in the field. We show in this paper that the problem of counting rate estimation can be interpreted as a sparse regression problem. We suggest a post-processed, non-negative, version of the Least Absolute Shrinkage and Selection Operator (LASSO) to estimate the photon arrival times. The main difficulty in this problem is that no theoretical conditions can guarantee consistency in sparsity of LASSO, because the dictionary is not ideal and the signal is sampled. We therefore derive theoretical conditions and bounds which illustrate that the proposed method can none the less provide a good, close to the best attainable, estimate of the counting rate activity. The good performances of the proposed approach are studied on simulations and real datasets."
51,51,In this paper we complete the proof of Ryba's modular moonshine conjectures. We do this by applying Hodge theory to the cohomology of the monster Lie algebra over the ring of p-adic integers in order to calculate the Tate cohomology groups of elements of the monster acting on the monster vertex algebra.
52,52,"We report on the formation and development of the photonic band gap in two-dimensional 8-, 10- and 12-fold symmetry quasicrystalline lattices of low index contrast. Finite size structures made of dielectric cylindrical rods were studied and measured in the microwave region, and their properties compared with a conventional hexagonal crystal. Band gap characteristics were investigated by changing the direction of propagation of the incident beam inside the crystal. Various angles of incidence from 0 \degree to 30\degree were used in order to investigate the isotropic nature of the band gap. The arbitrarily high rotational symmetry of aperiodically ordered structures could be practically exploited to manufacture isotropic band gap materials, which are perfectly suitable for hosting waveguides or cavities."
53,53,"Tavenas has recently proved that any n^{O(1)}-variate and degree n polynomial in VP can be computed by a depth-4 circuit of size 2^{O(\sqrt{n}\log n)}. So to prove VP not equal to VNP, it is sufficient to show that an explicit polynomial in VNP of degree n requires 2^{\omega(\sqrt{n}\log n)} size depth-4 circuits. Soon after Tavenas's result, for two different explicit polynomials, depth-4 circuit size lower bounds of 2^{\Omega(\sqrt{n}\log n)} have been proved Kayal et al. and Fournier et al. In particular, using combinatorial design Kayal et al.\ construct an explicit polynomial in VNP that requires depth-4 circuits of size 2^{\Omega(\sqrt{n}\log n)} and Fournier et al.\ show that iterated matrix multiplication polynomial (which is in VP) also requires 2^{\Omega(\sqrt{n}\log n)} size depth-4 circuits.   In this paper, we identify a simple combinatorial property such that any polynomial f that satisfies the property would achieve similar circuit size lower bound for depth-4 circuits. In particular, it does not matter whether f is in VP or in VNP. As a result, we get a very simple unified lower bound analysis for the above mentioned polynomials.   Another goal of this paper is to compare between our current knowledge of depth-4 circuit size lower bounds and determinantal complexity lower bounds. We prove the that the determinantal complexity of iterated matrix multiplication polynomial is \Omega(dn) where d is the number of matrices and n is the dimension of the matrices. So for d=n, we get that the iterated matrix multiplication polynomial achieves the current best known lower bounds in both fronts: depth-4 circuit size and determinantal complexity. To the best of our knowledge, a \Theta(n) bound for the determinantal complexity for the iterated matrix multiplication polynomial was known only for constant d>1 by Jansen."
54,54,"We propose an axiomatic generic framework for modelling weak memory. We show how to instantiate this framework for SC, TSO, C++ restricted to release-acquire atomics, and Power. For Power, we compare our model to a preceding operational model in which we found a flaw. To do so, we define an operational model that we show equivalent to our axiomatic model.   We also propose a model for ARM. Our testing on this architecture revealed a behaviour later acknowledged as a bug by ARM, and more recently 33 additional anomalies.   We offer a new simulation tool, called herd, which allows the user to specify the model of his choice in a concise way. Given a specification of a model, the tool becomes a simulator for that model. The tool relies on an axiomatic description; this choice allows us to outperform all previous simulation tools. Additionally, we confirm that verification time is vastly improved, in the case of bounded model-checking.   Finally, we put our models in perspective, in the light of empirical data obtained by analysing the C and C++ code of a Debian Linux distribution. We present our new analysis tool, called mole, which explores a piece of code to find the weak memory idioms that it uses."
55,55,"The automobile is always a point of interest where new technology has been deployed. Because of this interest, human-vehicle interaction has been an appealing area for much research in recent years. The current in-vehicle design has been improved but still possesses some of the design from the traditional interaction style. In this paper, we propose a new user-oriented model for in-vehicle interaction model known as i-Interaction. The i-Interaction model provides user with an intuitive approach to interact with the In-Vehicle Information System (IVIS) by the keypad entry. It is the intent that the proposed usability testing for this model will help improve the way research and development is implemented from this topic. This model does not only provide the user with a direct interaction in vehicles but also introduce a new prospective that other research has not addressed."
56,56,"The validation of data from sensors has become an important issue in the operation and control of modern industrial plants. One approach is to use knowledge based techniques to detect inconsistencies in measured data. This article presents a probabilistic model for the detection of such inconsistencies. Based on probability propagation, this method is able to find the existence of a possible fault among the set of sensors. That is, if an error exists, many sensors present an apparent fault due to the propagation from the sensor(s) with a real fault. So the fault detection mechanism can only tell if a sensor has a potential fault, but it can not tell if the fault is real or apparent. So the central problem is to develop a theory, and then an algorithm, for distinguishing real and apparent faults, given that one or more sensors can fail at the same time. This article then, presents an approach based on two levels: (i) probabilistic reasoning, to detect a potential fault, and (ii) constraint management, to distinguish the real fault from the apparent ones. The proposed approach is exemplified by applying it to a power plant model."
57,57,In here we define the concept of fibered symmetric bimonoidal categories. These are roughly speaking fibered categories D->C whose fibers are symmetric monoidal categories parametrized by C and such that both D and C have a further structure of a symmetric monoidal category that satisfy certain coherences that we describe. Our goal is to show that we can correspond to a fibered symmetric bimonoidal category an E_{\infty}-ring spectrum in a functorial way.
58,58,"Beam tail effect of multi-bunches will influence the electron beam performances in high intensity thermionic RF gun. Beam dynamic calculations that illustrate the working states of single and multi-pulse fed-in of performance-enhanced EC-ITC (External Cathode Independent Tunable Cavity) RF gun for FEL (Free Electron Laser) injector are performed to estimate extracted bunch properties. By using both Parmela and homemade MATLAB codes, the effects of single beam tail as well as interactions of multi-pulses are analyzed, where ring-based electron algorithm is adopted to calculated RF fields and space charge field. Furthermore, the procedure of unexpected deviated-energy particles mixed with effective bunch head is described by MATLAB code as well. As a result, performance-enhanced EC-ITC RF gun is proved to have the capability to extract continual stable bunches which are suitable for high requirement THz-FEL."
59,59,"In a recent study by Ginther et al., the probability of receiving a U.S. National Institutes of Health (NIH) RO1 award was related to the applicant's race/ethnicity. The results indicate black/African-American applicants were 10% less likely than white peers to receive an award, after controlling for background and qualifications. It has generated a widespread debate regarding the unfairness of the NIH grant review process and its correction. In this paper, the work by Ginther et al. was augmented by pairing analysis, axiomatically-individualized productivity and normalized funding success measurement. Although there are racial differences in R01 grant success rates, normalized figures of merit for funding success explain the discrepancy. The suggested ""leverage points for policy intervention"" are in question and require deeper and more thorough investigations. Further adjustments in policies to remove racial disparity should be made more systematically for equal opportunity, rather than being limited to the NIH review process."
60,60,We construct isospectral non isometric metrics on real and complex projective space. We recall the construction using isometric torus actions by Carolyn Gordon in chapter 2. In chapter 3 we will recall some facts about complex projective space. In chapter 4 we build the isospectral metrics. Chapter 5 is devoted to the non isometry proof of the metrics built in chapter 4. In chapter 6 isospectral metrics on real projective space are derived from metrics on the sphere.
61,61,"Nonresponse is present in almost all surveys and can severely bias estimates. It is usually distinguished between unit and item nonresponse: in the former, we completely fail to have information from a unit selected in the sample, while in the latter, we observe only part of the information on the selected unit. Unit nonresponse is usually dealt with by reweighting: each unit selected in the sample has associated a sampling weight and an unknown response probability; the initial sampling weight is multiplied by the inverse of estimated response probability. Item nonresponse is usually dealt with by imputation. By noting that for a particular survey variable, we just have observed and unobserved values, in this work we exploit the connection between unit and item nonresponse. In particular, we assume that the factors that drive unit response are the same as those that drive item response on selected variables of interest. Response probabilities are then estimated by using a logistic regression with a latent covariate that measures such will to respond and that can explain part of the unknown behavior of a unit to participate in the survey. The latent covariate is estimated using latent trait models. Such approach is particularly relevant for sensitive items and, therefore, can handle non-ignorable nonresponse. Auxiliary information known for both respondents and nonrespondents can be included either in the latent variable model or in the logistic model. The approach can be also used when auxiliary information is not available, and we focus here on this case. The theoretical properties of the proposed estimators are sketched and simulations studies are conducted to illustrate their finite size sample performance."
62,62,"A simple variational Lagrangian is proposed for the time development of an arbitrary density matrix, employing the ""factorization"" of the density. Only the ""kinetic energy"" appears in the Lagrangian. The formalism applies to pure and mixed state cases, the Navier-Stokes equations of hydrodynamics, transport theory, etc. It recaptures the Least Dissipation Function condition of Rayleigh-Onsager {\bf and in practical applications is flexible}. The variational proposal is tested on a two level system interacting that is subject, in one instance, to an interaction with a single oscillator and, in another, that evolves in a dissipative mode."
63,63,"Hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs."
64,64,"Let r be a real number, 0<r<1, given as a dual number. With the sequence of ""0"", ""1"" we define a path in a hexagonal lattice. Relations between the properties of r and its path are considered. Generalisations to other bases and lattices."
65,65,"As it is well-known, the year 2005 has been the centenary of the ""annus mirabilis"" (1905) during which Albert Einstein published four fundamental papers of his. But already in 1979, for the centenary of Einstein's birth, the world celebrated his monumantal work. In Italy too, there appeared scientific books, and many semi-popularization (or popularization) articles. The present paper represents a talk delivered in Italian, at the invitation of the Nobel Foundation (Sanremo, IM; Italy), in time for its publication in 1979. This article has been however reprinted, much more recently, in 2002, by the ""Centro DIEA"", Faculty of Engineering, University of Bologna, Bologna, Italy [and its source-file, in html, has been prepared with DIEA's collaboration]. We present here a description of the human, philosophycal and scientific background, starting from which Einstein produced his amazing results: Indeed, we try to show why Einstein's writings today are still so important not only for pure physics (and technology!), but also for our epistemological understanding of the procedures followed by science, and by our own mind, in their development, as well as for our philosophical views about the world we live in; without forgetting the teachings that come (or should come) from Einstein's life and claims for modern pedagogy."
66,66,This is an expository account of the edge eigenvalue distributions in random matrix theory and their application in multivariate statistics. The emphasis is on the Painlev\'e representations of these distributions.
67,67,"A classic problem of the motion of a point mass (projectile) thrown at an angle to the horizon is reviewed. The air drag force is taken into account with the drag factor assumed to be constant. Analytic approach is used for investigation. The problem of finding an optimal angle of launching a point mass in a medium with quadratic drag force is considered. An equation for determining a value of this angle is obtained. After finding the optimal angle of launching, eight main parameters of the point mass motion are analytically determined. These parameters are used to construct analytically six main functional relationships of the problem. Simple analytic formulas are used to solve two problems of optimization aimed to maximize the flight range of a point mass and minimize the initial speed of the point mass for getting to the given point on the plane. The motion of a baseball is presented as an example."
68,68,"We continue the study of the following Hamiltonian equation on the Hardy space of the circle, $$i\partial _tu=\Pi(|u|^2u)\ ,$$ where $\Pi $ denotes the Szeg\""o projector. This equation can be seen as a toy model for totally non dispersive evolution equations. In a previous work, we proved that this equation admits a Lax pair, and that it is completely integrable. In this paper, we construct the action-angle variables, which reduces the explicit resolution of the equation to a diagonalisation problem. As a consequence, we solve an inverse spectral problem for Hankel operators. Moreover, we establish the stability of the corresponding invariant tori. Furthermore, from the explicit formulae, we deduce the classification of orbitally stable and unstable traveling waves."
69,69,"We consider the problem of selecting covariates in spatial linear models with Gaussian process errors. Penalized maximum likelihood estimation (PMLE) that enables simultaneous variable selection and parameter estimation is developed and, for ease of computation, PMLE is approximated by one-step sparse estimation (OSE). To further improve computational efficiency, particularly with large sample sizes, we propose penalized maximum covariance-tapered likelihood estimation (PMLE$_{\mathrm{T}}$) and its one-step sparse estimation (OSE$_{\mathrm{T}}$). General forms of penalty functions with an emphasis on smoothly clipped absolute deviation are used for penalized maximum likelihood. Theoretical properties of PMLE and OSE, as well as their approximations PMLE$_{\mathrm{T}}$ and OSE$_{\mathrm{T}}$ using covariance tapering, are derived, including consistency, sparsity, asymptotic normality and the oracle properties. For covariance tapering, a by-product of our theoretical results is consistency and asymptotic normality of maximum covariance-tapered likelihood estimates. Finite-sample properties of the proposed methods are demonstrated in a simulation study and, for illustration, the methods are applied to analyze two real data sets."
70,70,"The structure of a network dramatically affects the spreading phenomena unfolding upon it. The contact distribution of the nodes has long been recognized as the key ingredient in influencing the outbreak events. However, limited knowledge is currently available on the role of the weight of the edges on the persistence of a pathogen. At the same time, recent works showed a strong influence of temporal network dynamics on disease spreading. In this work we provide an analytical understanding, corroborated by numerical simulations, about the conditions for infected stable state in weighted networks. In particular, we reveal the role of heterogeneity of edge weights and of the dynamic assignment of weights on the ties in the network in driving the spread of the epidemic. In this context we show that when weights are dynamically assigned to ties in the network an heterogeneous distribution is able to hamper the diffusion of the disease, contrary to what happens when weights are fixed in time."
71,71,We summarise the main results from a number of our recent articles on the subject of probabilistic temperature forecasting.
72,72,"We derive and study SQMC (Sequential Quasi-Monte Carlo), a class of algorithms obtained by introducing QMC point sets in particle filtering. SQMC is related to, and may be seen as an extension of, the array-RQMC algorithm of \cite{LEcuyer2006}. The complexity of SQMC is $\bigO(N\log N)$, where $N$ is the number of simulations at each iteration, and its error rate is smaller than the Monte Carlo rate $\bigO_P(N^{-1/2})$. The only requirement to implement SQMC is the ability to write the simulation of particle $\bx_{t}^{n}$ given $\bx_{t-1}^{n}$ as a deterministic function of $\bx_{t-1}^{n}$ and a fixed number of uniform variates. We show that SQMC is amenable to the same extensions as standard SMC, such as forward smoothing, backward smoothing, unbiased likelihood evaluation, and so on. In particular, SQMC may replace SMC within a PMCMC (particle Markov chain Monte Carlo) algorithm. We establish several convergence results. We provide numerical evidence that SQMC may significantly outperform SMC in practical scenarios."
73,73,"The nonlinear diffusion in multicomponent liquids under chemical reactions influence has been studied. The theory is applied to the analysis of mass transfer in a solution of acetone-benzene. It has been shown, that the creation of molecular complexes should be taken into account for the explanation of the experimental data on concentration dependence of diffusion coefficients. The matrix of mutual diffusivities has been found and effective parameters of the system have been computed."
74,74,"The geometric objects of study in this paper are K3 surfaces which admit a polarization by the unique even unimodular lattice of signature (1,17). A standard Hodge-theoretic observation about this special class of K3 surfaces is that their polarized Hodge structures are identical with the polarized Hodge structures of abelian surfaces that are cartesian products of elliptic curves. Earlier work of the first two authors gives an explicit normal form and construction of the moduli space for these surfaces. In the present work, this normal form is used to derive Picard-Fuchs differential equations satisfied by periods of these surfaces. We also investigate the subloci of the moduli space on which the polarization is enhanced. In these cases, we derive information about the Picard-Fuchs differential equations satisfied by periods of these subfamilies, and we relate this information to the theory of genus zero quotients of the upper half-plane by Moonshine groups. For comparison, we also examine the analogous theory for elliptic curves in Weierstrass form."
75,75,A theoretical mechanism of laminar-turbulent transition originated from the deceleration of fluid streams on the walls of the channel or pipe is proposed. For Poiseuille flow an analytical expression relating the critical Reynolds number with the degree of disturbance of the flow is derived.
76,76,"The long-standing assumption that the stellar initial mass function (IMF) is universal has recently been challenged by a number of observations. Several studies have shown that a ""heavy"" IMF (e.g., with a Salpeter-like abundance of low mass stars and thus normalisation) is preferred for massive early-type galaxies, while this IMF is inconsistent with the properties of less massive, later-type galaxies. These discoveries motivate the hypothesis that the IMF may vary (possibly very slightly) across galaxies and across components of individual galaxies (e.g. bulges vs discs). In this paper we use a sample of 19 late-type strong gravitational lenses from the SWELLS survey to investigate the IMFs of the bulges and discs in late-type galaxies. We perform a joint analysis of the galaxies' total masses (constrained by strong gravitational lensing) and stellar masses (constrained by optical and near-infrared colours in the context of a stellar population synthesis [SPS] model, up to an IMF normalisation parameter). Using minimal assumptions apart from the physical constraint that the total stellar mass within any aperture must be less than the total mass within the aperture, we find that the bulges of the galaxies cannot have IMFs heavier (i.e. implying high mass per unit luminosity) than Salpeter, while the disc IMFs are not well constrained by this data set. We also discuss the necessity for hierarchical modelling when combining incomplete information about multiple astronomical objects. This modelling approach allows us to place upper limits on the size of any departures from universality. More data, including spatially resolved kinematics (as in paper V) and stellar population diagnostics over a range of bulge and disc masses, are needed to robustly quantify how the IMF varies within galaxies."
77,77,"Researchers are developing mobile sensing platforms to facilitate public awareness of environmental conditions. However, turning such awareness into practical community action and political change requires more than just collecting and presenting data. To inform research on mobile environmental sensing, we conducted design fieldwork with government, private, and public interest stakeholders. In parallel, we built an environmental air quality sensing system and deployed it on street sweeping vehicles in a major U.S. city; this served as a ""research vehicle"" by grounding our interviews and affording us status as environmental action researchers. In this paper, we present a qualitative analysis of the landscape of environmental action, focusing on insights that will help researchers frame meaningful technological interventions."
78,78,"In this paper it is shown that statistical mechanics in the form of thermodynamic entropy can be used as a measure of the severity of individual injuries (AIS), and that the correct way to account for multiple injuries is to sum the entropies. It is further shown that summing entropies according to the Planck-Boltzmann (P-B) definition of entropy is formally the same as ISS, which is why ISS works. Approximate values of the probabilities of fatality are used to calculate the Gibb's entropy, which is more accurate than the P-B entropy far from equilibrium, and are shown to be again proportional to ISS. For the categorisation of injury using entropies it is necessary to consider the underlying entropy of the individuals morbidity to which is added the entropy of trauma, which then may result in death. Adding in the underlying entropy and summing entropies of all AIS3+ values gives a more extended scale than ISS, and so entropy is considered the preferred measure. A small scale trial is conducted of these concepts using the APROSYS In-Depth Pedestrian database, and the differences between the measures are illustrated. It is shown that adopting an entropy approach to categorising injury severity highlights the position of the elderly, who have a reduced physiological reserve to resist further traumatic onslaught. There are other informational entropy-like measures, here called i-entropy, which can also be used to classify injury severity, which are outlined. A large scale trial of these various entropy or i-entropy measures needs to be conducted to assess the usefulness of the measures. In the meantime, an age compensated ISS measure such as ASCOT or TRISS is recommended."
79,79,"We present elastic and inelastic spin-changing cross sections for cold and ultracold NH($X\,^3\Sigma^-$) + NH($X\,^3\Sigma^-$) collisions, obtained from full quantum scattering calculations on an accurate \textit{ab initio} quintet potential-energy surface. Although we consider only collisions in zero field, we focus on the cross sections relevant for magnetic trapping experiments. It is shown that evaporative cooling of both fermionic $^{14}$NH and bosonic $^{15}$NH is likely to be successful for hyperfine states that allow for s-wave collisions. The calculated cross sections are very sensitive to the details of the interaction potential, due to the presence of (quasi-)bound state resonances. The remaining inaccuracy of the \textit{ab initio} potential-energy surface therefore gives rise to an uncertainty in the numerical cross-section values. However, based on a sampling of the uncertainty range of the \textit{ab initio} calculations, we conclude that the exact potential is likely to be such that the elastic-to-inelastic cross-section ratio is sufficiently large to achieve efficient evaporative cooling. This likelihood is only weakly dependent on the size of the channel basis set used in the scattering calculations."
80,80,"We prove that for a topological space X with the property that $H_p(U)=0$ for $p\geq d$ and every open subset $U$ of $X$, a finite family of open sets in $X$ has nonempty intersection if for any subfamily of size $j$, $1\leq j \leq d+1$, the $(d-j)$-dimensional homology group of its intersection is zero. We use this theorem to prove new results concerning transversal affine planes to families of convex sets."
81,81,"We investigate a new class of nonlinear control systems of O.D.E., which are not feedback linearizable in general. Our class is a generalization of the well-known feedback linearizable systems, and moreover it is a generalization of the triangular (or pure-feedback) forms studied before. The definition of our class is global, and coordinate-free, which is why the problem of the equivalence is solved for our class in the whole state space at the very beginning. The goal of this paper is to prove the global controllability of our nonlinear systems. We propose to treat our class as a new canonical form which is a nonlinear global analog of the Brunovsky canonical form on the one hand, and is a global and coordinate-free generalization of the triangular form on the other hand."
82,82,"An experiment investigating the angle of Cerenkov light emitted by 3-MeV electrons traversing an acrylic detector has been developed for use in the advanced physics laboratory course at the University of Rochester. In addition to exploring the experimental phenomena of Cerenkov radiation and total internal reflection, the experiment introduces students to several experimental techniques used in actual high energy and nuclear physics experiments, as well as to analysis techniques involving Poisson statistics. [to be published in Am. J. Phys. 67 (Oct/Nov 1999).]"
83,83,"Within the quasichemical approach, the hydration free energy of an ion is decomposed into a chemical term accounting for ion specific ion-water interactions within the coordination sphere and nonspecific contributions accounting for packing (excluded volume) and long range interactions. The change in the chemical term with a change in the radius of the coordination sphere is the compressive force exerted by the bulk solvent medium on the surface of the coordination sphere. For the Na+, K+, F-, and Cl- ions considered here this compressive force becomes equal for similarly charged ions for coordination radii of about 0.39 nm, not much larger than a water molecule. These results show that ion specific effects are short ranged and arise primarily due to differences in the local ion-water interactions."
84,84,"Kinetic instabilities in weakly collisional, high beta plasmas are investigated using two-dimensional hybrid expanding box simulations with Coulomb collisions modeled through the Langevin equation (corresponding to the Fokker-Planck one). The expansion drives a parallel or perpendicular temperature anisotropy (depending on the orientation of the ambient magnetic field). For the chosen parameters the Coulomb collisions are important with respect to the driver but are not strong enough to keep the system stable with respect to instabilities driven by the proton temperature anisotropy. In the case of the parallel temperature anisotropy the dominant oblique fire hose instability efficiently reduces the anisotropy in a quasilinear manner. In the case of the perpendicular temperature anisotropy the dominant mirror instability generates coherent compressive structures which scatter protons and reduce the temperature anisotropy. For both the cases the instabilities generate temporarily enough wave energy so that the corresponding (anomalous) transport coefficients dominate over the collisional ones and their properties are similar to those in collisionless plasmas."
85,85,"MaxEnt's variational principle, in conjunction with Shannon's logarithmic information measure, yields only exponential functional forms in straightforward fashion. In this communication we show how to overcome this limitation via the incorporation, into the variational process, of suitable dynamical information. As a consequence, we are able to formulate a somewhat generalized Shannonian Maximum Entropy approach which provides a unifying ""thermodynamic-like"" explanation for the scale-invariant phenomena observed in social contexts, as city-population distributions. We confirm the MaxEnt predictions by means of numerical experiments with random walkers, and compare them with some empirical data."
86,86,"We study the set $S_{ann-nc}$ of permutations of $\{1, ..., p+q \}$ which are non-crossing in an annulus with $p$ points marked on its external circle and $q$ points marked on its internal circle. The algebraic approach to $S_{ann-nc}$ goes by identifying three possible crossing patterns in an annulus, and by defining a permutation to be annular non-crossing when it does not display any of these patterns. We prove the annular counterpart for a ``geodesic condition'' shown by Biane to characterize non-crossing permutations in a disc. We point out that, as a consequence, annular non-crossing permutations appear in the description of the second order asymptotics for the joint moments of certain families (Wishart and GUE) of random matrices. We examine the relation between $S_{ann-nc}$ and the set $NC_{ann}$ of annular non-crossing partitions of $\{1, ..., p+q \}$, and observe that (unlike in the disc case) the natural map from $S_{ann-nc}$ onto $NC_{ann}$ has a pathology which prevents it from being injective."
87,87,"Testing for white noise has been well studied in the literature of econometrics and statistics. For most of the proposed test statistics, such as the well-known Box-Pierce's test statistic with fixed lag truncation number, the asymptotic null distributions are obtained under independent and identically distributed assumptions and may not be valid for the dependent white noise. Due to recent popularity of conditional heteroscedastic models (e.g., GARCH models), which imply nonlinear dependence with zero autocorrelation, there is a need to understand the asymptotic properties of the existing test statistics under unknown dependence. In this paper, we showed that the asymptotic null distribution of Box-Pierce's test statistic with general weights still holds under unknown weak dependence so long as the lag truncation number grows at an appropriate rate with increasing sample size. Further applications to diagnostic checking of the ARMA and FARIMA models with dependent white noise errors are also addressed. Our results go beyond earlier ones by allowing non-Gaussian and conditional heteroscedastic errors in the ARMA and FARIMA models and provide theoretical support for some empirical findings reported in the literature."
88,88,It is shown as experiments and theories about the nature of light led to the special theory of relativity. The most important facts for the emergence of the theory proposed by Einstein in 1905 are presented.
89,89,"The recent nuclear accident in Japan revealed the confusion and the inadequate knowledge of the citizens about the issues of nuclear energy, nuclear applications, radioactivity and their consequences In this work we present the first results of an ongoing study which aims to evaluate the knowledge and the views of Greek undergraduate students on the above issues. A web based survey was conducted and 131 students from TEI Piraeus answered a multiple choice questionnaire with questions of general interest on nuclear energy, nuclear applications, radioactivity and their consequences. The survey showed that students, like the general population, have a series of faulty views on general interest nuclear issues. Furthermore, the first results indicate that our educational system is not so effective as source of information on these issues in comparison to the media and internet"
90,90,"We propose a lexical account of action nominals, in particular of deverbal nominalisations, whose meaning is related to the event expressed by their base verb. The literature about nominalisations often assumes that the semantics of the base verb completely defines the structure of action nominals. We argue that the information in the base verb is not sufficient to completely determine the semantics of action nominals. We exhibit some data from different languages, especially from Romance language, which show that nominalisations focus on some aspects of the verb semantics. The selected aspects, however, seem to be idiosyncratic and do not automatically result from the internal structure of the verb nor from its interaction with the morphological suffix. We therefore propose a partially lexicalist approach view of deverbal nouns. It is made precise and computable by using the Montagovian Generative Lexicon, a type theoretical framework introduced by Bassac, Mery and Retor\'e in this journal in 2010. This extension of Montague semantics with a richer type system easily incorporates lexical phenomena like the semantics of action nominals in particular deverbals, including their polysemy and (in)felicitous copredications."
91,91,"Analysis how to use Internet influence to the process of political communication, marketing and the management of public relations, what kind of online communication methods are used by political parties, and to assess satisfaction, means of communication and the services they provide to their partys voters (people) and other interest groups and whether social networks can affect the political and economic changes in the state, and the political power of one party."
92,92,"In this paper we address the decision problem for a fragment of set theory with restricted quantification which extends the language studied in [4] with pair related quantifiers and constructs, in view of possible applications in the field of knowledge representation. We will also show that the decision problem for our language has a non-deterministic exponential time complexity. However, for the restricted case of formulae whose quantifier prefixes have length bounded by a constant, the decision problem becomes NP-complete. We also observe that in spite of such restriction, several useful set-theoretic constructs, mostly related to maps, are expressible. Finally, we present some undecidable extensions of our language, involving any of the operators domain, range, image, and map composition.   [4] Michael Breban, Alfredo Ferro, Eugenio G. Omodeo and Jacob T. Schwartz (1981): Decision procedures for elementary sublanguages of set theory. II. Formulas involving restricted quantifiers, together with ordinal, integer, map, and domain notions. Communications on Pure and Applied Mathematics 34, pp. 177-195"
93,93,"Equilibrium statistics of Hamiltonian systems is correctly described by the microcanonical ensemble. Classically this is the manifold of all points in the $N-$body phase space with the given total energy. Due to Boltzmann's principle, $e^S=tr(\delta(E-H))$, its geometrical size is related to the entropy $S(E,N,...)$. This definition does not invoke any information theory, no thermodynamic limit, no extensivity, and no homogeneity assumption, as are needed in conventional (canonical) thermo-statistics. Therefore, it describes the equilibrium statistics of extensive as well of non-extensive systems. Due to this fact it is the {\em fundamental} definition of any classical equilibrium statistics. It can address nuclei and astrophysical objects as well. All kind of phase transitions can be distinguished sharply and uniquely for even small systems. It is further shown that the second law is a natural consequence of the statistical nature of thermodynamics which describes all systems with the same -- redundant -- set of few control parameters simultaneously. It has nothing to do with the thermodynamic limit. It even works in systems which are by far {\em larger} than any thermodynamic ""limit""."
94,94,"A major challenge in testing software product lines is efficiency. In particular, testing a product line should take less effort than testing each and every product individually. We address this issue in the context of input-output conformance testing, which is a formal theory of model-based testing. We extend the notion of conformance testing on input-output featured transition systems with the novel concept of spinal test suites. We show how this concept dispenses with retesting the common behavior among different, but similar, products of a software product line."
95,95,"The existence of resonant enhanced transmission and collimation of light waves by subwavelength slits in metal films [for example, see T.W. Ebbesen et al., Nature (London) 391, 667 (1998) and H.J. Lezec et al., Science, 297, 820 (2002)] leads to the basic question: Can a light be enhanced and simultaneously localized in space and time by a subwavelength slit? To address this question, the spatial distribution of the energy flux of an ultrashort (femtosecond) wave-packet diffracted by a subwavelength (nanometer-size) slit was analyzed by using the conventional approach based on the Neerhoff and Mur solution of Maxwell's equations. The results show that a light can be enhanced by orders of magnitude and simultaneously localized in the near-field diffraction zone at the nm- and fs-scales. Possible applications in nanophotonics are discussed."
96,96,"This paper discusses a stylized communications problem where one wishes to transmit a real-valued signal x in R^n (a block of n pieces of information) to a remote receiver. We ask whether it is possible to transmit this information reliably when a fraction of the transmitted codeword is corrupted by arbitrary gross errors, and when in addition, all the entries of the codeword are contaminated by smaller errors (e.g. quantization errors).   We show that if one encodes the information as Ax where A is a suitable m by n coding matrix (m >= n), there are two decoding schemes that allow the recovery of the block of n pieces of information x with nearly the same accuracy as if no gross errors occur upon transmission (or equivalently as if one has an oracle supplying perfect information about the sites and amplitudes of the gross errors). Moreover, both decoding strategies are very concrete and only involve solving simple convex optimization programs, either a linear program or a second-order cone program. We complement our study with numerical simulations showing that the encoder/decoder pair performs remarkably well."
97,97,"With the development of nanotechnology, the measurement of electrical properties in local area of materials and devices has become a great need. Although a lot kind of scanning probe microscope have been developed for satisfying the requirement of nanotechnology, a microscope technique which can determine electrical properties in local area of materials and devices is not yet developed. Recently, microwave microscope has been an interest to many researchers, due to its potential in the evaluation of electrical properties of materials and devices. The advance of microwave is that the response of materials is directly relative to the electromagnetic properties of materials. However, because of the problem of the structure of probes, nanometer-scale resolution has not been successful. To achieve the goal, a new structure microwave probe is required. In this paper, we report a nanostructural microwave probe. To restrain the attenuation of microwave in the probe, GaAs was used as the substrate of the probe. To obtain the desired structure, wet etching was used to fabricate the probe. Different with the dry etching, a side-etching will occur under the etching mask. Utilizing this property, a micro tip can be fabricated by etching a wafer, of which a small mask was introduced on the surface in advance."
98,98,"In the framework of noisy quantum homodyne tomography with efficiency parameter $1/2 < \eta \leq 1$, we propose a novel estimator of a quantum state whose density matrix elements $\rho_{m,n}$ decrease like $Ce^{-B(m+n)^{r/ 2}}$, for fixed $C\geq 1$, $B>0$ and $0<r\leq 2$. On the contrary to previous works, we focus on the case where $r$, $C$ and $B$ are unknown. The procedure estimates the matrix coefficients by a projection method on the pattern functions, and then by soft-thresholding the estimated coefficients.   We prove that under the $\mathbb{L}_2$ -loss our procedure is adaptive rate-optimal, in the sense that it achieves the same rate of conversgence as the best possible procedure relying on the knowledge of $(r,B,C)$. Finite sample behaviour of our adaptive procedure are explored through numerical experiments."
99,99,"Answer-Set Programming (ASP) is an established declarative programming paradigm. However, classical ASP lacks subprogram calls as in procedural programming, and access to external computations (like remote procedure calls) in general. The feature is desired for increasing modularity and---assuming proper access in place---(meta-)reasoning over subprogram results. While HEX-programs extend classical ASP with external source access, they do not support calls of (sub-)programs upfront. We present nested HEX-programs, which extend HEX-programs to serve the desired feature, in a user-friendly manner. Notably, the answer sets of called sub-programs can be individually accessed. This is particularly useful for applications that need to reason over answer sets like belief set merging, user-defined aggregate functions, or preferences of answer sets."