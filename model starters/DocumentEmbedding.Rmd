---
title: "Document embedding trial"
author: "Keli Chiu"
date: "15/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(textdata)
library(tidyverse)
library(widyr)
library(tidytext)
library(SnowballC)
```

## Can we use word embedding for document similarity and document classification?

Yes! I learned that similar to represent word in vectors, we can represent documents in vectors, and it's referred as document embedding. Once we have the document vectors, we can then find similar documents and also do classification.

The most mentioned way to do document embedding is Paragraph Vector (widely referred as Doc2Vec)), which is based on Word2Vec and proposed by the same researchers. (The related article is here: https://arxiv.org/abs/1405.4053) "the paragraph vector is concatenated or averaged with local context word vectors", and then an addtional vector is added for the document(paragraph) ID. In the end, the vectors of the document can be used as the features of the document. The features can be fed to machine learning algorithms, including classification. 

Another way that's also frequently proposed is to aggregate all the word vectors in a document as the representation of the document. After obtaining hte aggregation of the word embeddings (usually the sum or the average), we can then calculate the cosine distance between the document vectors in order to detect similarity. 

## What are other ways to do document embedding?

- **Topic modeling with LDA**, the documents are represented by topic distributions
- **Sentence-BERT (SBERT)**
- **TF-IDF**

## Example of document embedding 

Doc2Vec is available in Python but doesn't seem to be available in R. The following example is the aggregation of pretrained GloVe word vectors, following the tutorial of Julia Silge: https://smltar.com/embeddings.html# In the tutorial, Julia sums GloVe word embeddings in a document as the representation vectors of that document. The training documents are replaced with paper abstracts downloaded from Kaggle: https://www.kaggle.com/nikhilmittal/research-paper-abstracts

```{r,warning=FALSE}
# Tokenize the abstract
abstracts <- read_csv("abstracts_101.csv")
tidy_abstracts <- abstracts %>%
  select(id, abstract) %>%
  unnest_tokens(word, abstract) %>%
  add_count(word) %>%
  filter(n >= 50) %>%
  select(-n)
```

```{r}
# Download the glove word embeddings
glove6b <- embedding_glove6b(dimensions = 100)
glove6b
```

```{r}
# Change the word embedding format to be more tidy
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension"
  ) %>%
  rename(item1 = token)
```

```{r}
# Change the word embedding format to be more tidy
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension"
  ) %>%
  rename(item1 = token)
```

```{r}
# Change the word embedding format to be more tidy
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension"
  ) %>%
  rename(item1 = token)
```

```{r}
# Document embedding
## GloVe embedding doens't handle unseen words, so we have to match 
## the abstracts and GloVe embedding to have the same set of vocabulary
word_matrix <- tidy_abstracts %>%
  inner_join(tidy_glove %>%
               distinct(item1) %>%
               rename(word = item1)) %>%
  count(id, word)
doc_ids <- distinct(word_matrix, id) # Retrieve the document ID
word_matrix_cast <-cast_sparse(word_matrix, id, word, n)

glove_matrix <- tidy_glove %>%
  inner_join(tidy_abstracts %>%
               distinct(word) %>%
               rename(item1 = word)) %>%
  cast_sparse(item1, dimension, value)

## Make the document matrix that contains document vectors
doc_matrix <- word_matrix_cast %*% glove_matrix
doc_matrix <- as.matrix(doc_matrix)
doc_matrix <- as.data.frame(doc_matrix)
doc_matrix <- cbind(doc_ids,doc_matrix ) # Put the document IDs back
doc_matrix
```
Up to this step, a document matrix with the corresponding vectors that represents the documentation is obtained. We can then perform matrix algera, or use the vectors to feed in machine learning algorithms for similarity detection or classfication. Further investigation is needed to perform the tasks!
